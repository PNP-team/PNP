diff --git a/third_party/blst b/third_party/blst
deleted file mode 160000
index f1097f5cd92..00000000000
--- a/third_party/blst
+++ /dev/null
@@ -1 +0,0 @@
-Subproject commit f1097f5cd9234e1cfba9d4ad63643973bc55bdd0
diff --git a/torch/csrc/jit/runtime/static/generated_ops.cpp b/torch/csrc/jit/runtime/static/generated_ops.cpp
index ffd629bc5f0..af61ee72a00 100644
--- a/torch/csrc/jit/runtime/static/generated_ops.cpp
+++ b/torch/csrc/jit/runtime/static/generated_ops.cpp
@@ -5280,5 +5280,5 @@ REGISTER_NATIVE_OPERATOR_FUNCTOR(
       LogAndDumpSchema(n);
       return nullptr;
     });
-    
+
 } // namespace torch::jit
diff --git a/torch/csrc/jit/tensorexpr/types.cpp b/torch/csrc/jit/tensorexpr/types.cpp
index 448fae3818c..c01504eb83e 100644
--- a/torch/csrc/jit/tensorexpr/types.cpp
+++ b/torch/csrc/jit/tensorexpr/types.cpp
@@ -99,76 +99,6 @@ std::string Dtype::ToCppString() const {
       return "qint8";
     case ScalarType::QUInt8:
       return "quint8";
-    case ScalarType::Field64:
-      return "field64";
-    case ScalarType::BigInteger:
-      return "big_integer";
-    case ScalarType::FiniteField:
-      return "finite_field";
-    case ScalarType::ALT_BN128_Fr_G1_Base:
-      return "ALT_BN128, Fr, G1, Base>";
-    case ScalarType::ALT_BN128_Fr_G2_Base:
-      return "ALT_BN128, Fr, G2, Base>";
-    case ScalarType::ALT_BN128_Fq_G1_Base:
-      return "ALT_BN128, Fq, G1, Base>";
-    case ScalarType::ALT_BN128_Fq_G2_Base:
-      return "ALT_BN128, Fq, G2, Base>";
-    case ScalarType::BLS12_377_Fr_G1_Base:
-      return "BLS12_377, Fr, G1, Base>";
-    case ScalarType::BLS12_377_Fr_G2_Base:
-      return "BLS12_377, Fr, G2, Base>";
-    case ScalarType::BLS12_377_Fq_G1_Base:
-      return "<BLS12_377, Fq, G1, Base>";
-    case ScalarType::BLS12_377_Fq_G2_Base:
-      return "<BLS12_377, Fq, G2, Base>";
-    case ScalarType::BLS12_381_Fr_G1_Base:
-      return "<BLS12_381, Fr, G1, Base>";
-    case ScalarType::BLS12_381_Fr_G2_Base:
-      return "<BLS12_381, Fr, G2, Base>";
-    case ScalarType::BLS12_381_Fq_G1_Base:
-      return "<BLS12_381, Fq, G1, Base>";
-    case ScalarType::BLS12_381_Fq_G2_Base:
-      return "<BLS12_381, Fq, G2, Base>";
-    case ScalarType::MNT4753_Fr_G1_Base:
-      return "<MNT4753, Fr, G1, Base>";
-    case ScalarType::MNT4753_Fr_G2_Base:
-      return "<MNT4753, Fr, G2, Base>";
-    case ScalarType::MNT4753_Fq_G1_Base:
-      return "<MNT4753, Fq, G1, Base>";
-    case ScalarType::MNT4753_Fq_G2_Base:
-      return "<MNT4753, Fq, G2, Base>";
-    case ScalarType::ALT_BN128_Fr_G1_Mont:
-      return "<ALT_BN128, Fr, G1, Mont>";
-    case ScalarType::ALT_BN128_Fr_G2_Mont:
-      return "<ALT_BN128, Fr, G2, Mont>";
-    case ScalarType::ALT_BN128_Fq_G1_Mont:
-      return "<ALT_BN128, Fq, G1, Mont>";
-    case ScalarType::ALT_BN128_Fq_G2_Mont:
-      return "<ALT_BN128, Fq, G2, Mont>";
-    case ScalarType::BLS12_377_Fr_G1_Mont:
-      return "<BLS12_377, Fr, G1, Mont>";
-    case ScalarType::BLS12_377_Fr_G2_Mont:
-      return "<BLS12_377, Fr, G2, Mont>";
-    case ScalarType::BLS12_377_Fq_G1_Mont:
-      return "<BLS12_377, Fq, G1, Mont>";
-    case ScalarType::BLS12_377_Fq_G2_Mont:
-      return "<BLS12_377, Fq, G2, Mont>";
-    case ScalarType::BLS12_381_Fr_G1_Mont:
-      return "<BLS12_381, Fr, G1, Mont>";
-    case ScalarType::BLS12_381_Fr_G2_Mont:
-      return "<BLS12_381, Fr, G2, Mont>";
-    case ScalarType::BLS12_381_Fq_G1_Mont:
-      return "<BLS12_381, Fq, G1, Mont>";
-    case ScalarType::BLS12_381_Fq_G2_Mont:
-      return "<BLS12_381, Fq, G2, Mont>";
-    case ScalarType::MNT4753_Fr_G1_Mont:
-      return "<MNT4753, Fr, G1, Mont>";
-    case ScalarType::MNT4753_Fr_G2_Mont:
-      return "<MNT4753, Fr, G2, Mont>";
-    case ScalarType::MNT4753_Fq_G1_Mont:
-      return "<MNT4753, Fq, G1, Mont>";
-    case ScalarType::MNT4753_Fq_G2_Mont:
-      return "<MNT4753, Fq, G2, Mont>";
     default:
       throw unsupported_dtype();
   }
diff --git a/torch/csrc/lazy/core/shape_inference.h b/torch/csrc/lazy/core/shape_inference.h
index 345174d670c..e243798cfc7 100644
--- a/torch/csrc/lazy/core/shape_inference.h
+++ b/torch/csrc/lazy/core/shape_inference.h
@@ -117,7 +117,6 @@ TORCH_API std::vector<torch::lazy::Shape> compute_shape_select_scatter(const at:
 TORCH_API std::vector<torch::lazy::Shape> compute_shape_diagonal_scatter(const at::Tensor & self, const at::Tensor & src, int64_t offset, int64_t dim1, int64_t dim2);
 TORCH_API std::vector<torch::lazy::Shape> compute_shape_slice_scatter_symint(const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step);
 TORCH_API std::vector<torch::lazy::Shape> compute_shape_as_strided_scatter_symint(const at::Tensor & self, const at::Tensor & src, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset);
-
 // clang-format on
 } // namespace lazy
 } // namespace torch
diff --git a/torch/csrc/utils/pybind.cpp b/torch/csrc/utils/pybind.cpp
index 87d878f801d..e3b3b5b8bc1 100644
--- a/torch/csrc/utils/pybind.cpp
+++ b/torch/csrc/utils/pybind.cpp
@@ -142,8 +142,6 @@ py::handle type_caster<c10::Scalar>::cast(
     return py::cast(scalar.toBool()).release();
   } else if (scalar.isComplex()) {
     return py::cast(scalar.toComplexDouble()).release();
-  } else if (scalar.isUnsigned()) {
-    return py::cast(scalar.toULong()).release();
   } else {
     TORCH_INTERNAL_ASSERT(0, "unrecognized scalar type ", scalar.type());
   }
diff --git a/torch/csrc/utils/python_numbers.h b/torch/csrc/utils/python_numbers.h
index fb4ffa2de79..0f276bdf373 100644
--- a/torch/csrc/utils/python_numbers.h
+++ b/torch/csrc/utils/python_numbers.h
@@ -28,16 +28,6 @@ inline PyObject* THPUtils_packUInt64(uint64_t value) {
   return PyLong_FromUnsignedLongLong(value);
 }
 
-inline PyObject* THPUtils_packUInt64List(const uint64_t* value, size_t num_elems) {
-  PyObject* pList = PyList_New(0);
-  for (size_t i = 0; i < num_elems; ++i) {
-    PyObject* pValue = PyLong_FromUnsignedLongLong(value[i]);
-    PyList_Append(pList, pValue);
-    Py_DECREF(pValue);
-  }
-  return pList;
-}
-
 inline PyObject* THPUtils_packDoubleAsInt(double value) {
   return PyLong_FromDouble(value);
 }
diff --git a/torch/csrc/utils/python_scalars.h b/torch/csrc/utils/python_scalars.h
index e9944e032a2..eb560b5a045 100644
--- a/torch/csrc/utils/python_scalars.h
+++ b/torch/csrc/utils/python_scalars.h
@@ -41,9 +41,6 @@ inline void store_scalar(void* data, at::ScalarType scalarType, PyObject* obj) {
     case at::kLong:
       *(int64_t*)data = unpackIntegral<int64_t>(obj, "int64");
       break;
-    case at::kULong:
-      *(uint64_t*)data = THPUtils_unpackUInt64(obj);
-      break;
     case at::kHalf:
       *(at::Half*)data =
           at::convert<at::Half, double>(THPUtils_unpackDouble(obj));
@@ -81,9 +78,6 @@ inline void store_scalar(void* data, at::ScalarType scalarType, PyObject* obj) {
       *(at::Float8_e4m3fn*)data =
           at::convert<at::Float8_e4m3fn, double>(THPUtils_unpackDouble(obj));
       break;
-    ALL_BIGINTEGER_CASE
-      *(uint64_t*)data = THPUtils_unpackUInt64(obj);
-      break;
     default:
       throw std::runtime_error("invalid type");
   }
@@ -101,8 +95,6 @@ inline PyObject* load_scalar(void* data, at::ScalarType scalarType) {
       return THPUtils_packInt64(*(int32_t*)data);
     case at::kLong:
       return THPUtils_packInt64(*(int64_t*)data);
-    case at::kULong:
-      return THPUtils_packUInt64(*(uint64_t*)data);
     case at::kHalf:
       return PyFloat_FromDouble(
           at::convert<double, at::Half>(*(at::Half*)data));
@@ -132,8 +124,6 @@ inline PyObject* load_scalar(void* data, at::ScalarType scalarType) {
     case at::kFloat8_e4m3fn:
       return PyFloat_FromDouble(
           at::convert<double, at::Float8_e4m3fn>(*(at::Float8_e4m3fn*)data));
-    ALL_BIGINTEGER_CASE
-      return THPUtils_packUInt64(*(uint64_t*)data);
     default:
       throw std::runtime_error("invalid type");
   }
diff --git a/torch/csrc/utils/tensor_dtypes.cpp b/torch/csrc/utils/tensor_dtypes.cpp
index d0bcfa03415..6897253a53e 100644
--- a/torch/csrc/utils/tensor_dtypes.cpp
+++ b/torch/csrc/utils/tensor_dtypes.cpp
@@ -28,8 +28,6 @@ std::pair<std::string, std::string> getDtypeNames(at::ScalarType scalarType) {
       return std::make_pair("int32", "int");
     case at::ScalarType::Long:
       return std::make_pair("int64", "long");
-    case at::ScalarType::ULong:
-      return std::make_pair("uint64", "unsigned long");
     case at::ScalarType::Short:
       return std::make_pair("int16", "short");
     case at::ScalarType::Half:
@@ -68,80 +66,6 @@ std::pair<std::string, std::string> getDtypeNames(at::ScalarType scalarType) {
       return std::make_pair("float8_e5m2", "");
     case at::ScalarType::Float8_e4m3fn:
       return std::make_pair("float8_e4m3fn", "");
-    case at::ScalarType::Field64:
-      return std::make_pair("field64", "");
-    case at::ScalarType::BigInteger:
-      return std::make_pair("big_integer", "");
-    case at::ScalarType::BigInteger_Mont:
-      return std::make_pair("big_integer_mont", "");
-    case at::ScalarType::FiniteField:
-      return std::make_pair("finite_field", "");
-    case at::ScalarType::NOT_CURVE:
-      return std::make_pair("should_never_use", "");
-    case at::ScalarType::ALT_BN128_Fr_G1_Base:
-      return std::make_pair("ALT_BN128_Fr_G1_Base", "");
-    case at::ScalarType::ALT_BN128_Fr_G2_Base:
-      return std::make_pair("ALT_BN128_Fr_G2_Base", "");
-    case at::ScalarType::ALT_BN128_Fq_G1_Base:
-      return std::make_pair("ALT_BN128_Fq_G1_Base", "");
-    case at::ScalarType::ALT_BN128_Fq_G2_Base:
-      return std::make_pair("ALT_BN128_Fq_G2_Base", "");
-    case at::ScalarType::BLS12_377_Fr_G1_Base:
-      return std::make_pair("BLS12_377_Fr_G1_Base", "");
-    case at::ScalarType::BLS12_377_Fr_G2_Base:
-      return std::make_pair("BLS12_377_Fr_G2_Base", "");
-    case at::ScalarType::BLS12_377_Fq_G1_Base:
-      return std::make_pair("BLS12_377_Fq_G1_Base", "");
-    case at::ScalarType::BLS12_377_Fq_G2_Base:
-      return std::make_pair("BLS12_377_Fq_G2_Base", "");
-    case at::ScalarType::BLS12_381_Fr_G1_Base:
-      return std::make_pair("BLS12_381_Fr_G1_Base", "");
-    case at::ScalarType::BLS12_381_Fr_G2_Base:
-      return std::make_pair("BLS12_381_Fr_G2_Base", "");
-    case at::ScalarType::BLS12_381_Fq_G1_Base:
-      return std::make_pair("BLS12_381_Fq_G1_Base", "");
-    case at::ScalarType::BLS12_381_Fq_G2_Base:
-      return std::make_pair("BLS12_381_Fq_G2_Base", "");
-    case at::ScalarType::MNT4753_Fr_G1_Base:
-      return std::make_pair("MNT4753_Fr_G1_Base", "");
-    case at::ScalarType::MNT4753_Fr_G2_Base:
-      return std::make_pair("MNT4753_Fr_G2_Base", "");
-    case at::ScalarType::MNT4753_Fq_G1_Base:
-      return std::make_pair("MNT4753_Fq_G1_Base", "");
-    case at::ScalarType::MNT4753_Fq_G2_Base:
-      return std::make_pair("MNT4753_Fq_G2_Base", "");
-    case at::ScalarType::ALT_BN128_Fr_G1_Mont:
-      return std::make_pair("ALT_BN128_Fr_G1_Mont", "");
-    case at::ScalarType::ALT_BN128_Fr_G2_Mont:
-      return std::make_pair("ALT_BN128_Fr_G2_Mont", "");
-    case at::ScalarType::ALT_BN128_Fq_G1_Mont:
-      return std::make_pair("ALT_BN128_Fq_G1_Mont", "");
-    case at::ScalarType::ALT_BN128_Fq_G2_Mont:
-      return std::make_pair("ALT_BN128_Fq_G2_Mont", "");
-    case at::ScalarType::BLS12_377_Fr_G1_Mont:
-      return std::make_pair("BLS12_377_Fr_G1_Mont", "");
-    case at::ScalarType::BLS12_377_Fr_G2_Mont:
-      return std::make_pair("BLS12_377_Fr_G2_Mont", "");
-    case at::ScalarType::BLS12_377_Fq_G1_Mont:
-      return std::make_pair("BLS12_377_Fq_G1_Mont", "");
-    case at::ScalarType::BLS12_377_Fq_G2_Mont:
-      return std::make_pair("BLS12_377_Fq_G2_Mont", "");
-    case at::ScalarType::BLS12_381_Fr_G1_Mont:
-      return std::make_pair("BLS12_381_Fr_G1_Mont", "");
-    case at::ScalarType::BLS12_381_Fr_G2_Mont:
-      return std::make_pair("BLS12_381_Fr_G2_Mont", "");
-    case at::ScalarType::BLS12_381_Fq_G1_Mont:
-      return std::make_pair("BLS12_381_Fq_G1_Mont", "");
-    case at::ScalarType::BLS12_381_Fq_G2_Mont:
-      return std::make_pair("BLS12_381_Fq_G2_Mont", "");
-    case at::ScalarType::MNT4753_Fr_G1_Mont:
-      return std::make_pair("MNT4753_Fr_G1_Mont", "");
-    case at::ScalarType::MNT4753_Fr_G2_Mont:
-      return std::make_pair("MNT4753_Fr_G2_Mont", "");
-    case at::ScalarType::MNT4753_Fq_G1_Mont:
-      return std::make_pair("MNT4753_Fq_G1_Mont", "");
-    case at::ScalarType::MNT4753_Fq_G2_Mont:
-      return std::make_pair("MNT4753_Fq_G2_Mont", "");
     default:
       throw std::runtime_error("Unimplemented scalar type");
   }
diff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp
index 8785cbc5840..11528d58626 100644
--- a/torch/csrc/utils/tensor_new.cpp
+++ b/torch/csrc/utils/tensor_new.cpp
@@ -440,7 +440,6 @@ Tensor internal_new_from_data(
     }
     pybind11::gil_scoped_release no_gil;
     maybe_initialize_cuda(device);
-
     // However, it is VERY important that we trace the to() call here (even
     // though the reason this is important is a hack).  Without *some* factory
     // function call that is traced at construction time, we will consider
diff --git a/torch/fx/graph.py b/torch/fx/graph.py
index 7c8a882a66c..7f86b0aff20 100644
--- a/torch/fx/graph.py
+++ b/torch/fx/graph.py
@@ -216,7 +216,6 @@ dtype_abbrs = {
     torch.int16: 'i16',
     torch.int32: 'i32',
     torch.int64: 'i64',
-    torch.uint64: 'u64',
     torch.bool: 'b8',
     torch.uint8: 'u8',
 }
diff --git a/torch/nn/functional.py b/torch/nn/functional.py
index ea70627181a..f4c75204dcd 100644
--- a/torch/nn/functional.py
+++ b/torch/nn/functional.py
@@ -5453,24 +5453,3 @@ def multi_head_attention_forward(
             # squeeze the output if input was unbatched
             attn_output = attn_output.squeeze(1)
         return attn_output, None
-
-# Below are for BigInteger
-def to_mont(input: Tensor, inplace: bool = False) -> Tensor:
-    r"""
-        Convert input to Montgomery domain. Only for Elliptic Curve.
-    """
-    if inplace:
-        result = torch.to_mont_(input)
-    else:
-        result = torch.to_mont(input)
-    return result
-
-def to_base(input: Tensor, inplace: bool = False) -> Tensor:
-    r"""
-        Convert input to base domain. Only for Elliptic Curve.
-    """
-    if inplace:
-        result = torch.to_base_(input)
-    else:
-        result = torch.to_base(input)
-    return result
